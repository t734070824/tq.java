2019-04-15

## 使用场景
1. 数据发布/订阅
2. 负载均衡
3. 命名服务
4. 分布式协调/通知
5. 集群管理
6. Master 选举
7. 分布式锁
8. 分布式队列

### 数据发布/订阅(所谓的配置中心)
1. 采用推拉结合的方式
    - 客户端注册
    - 数据变更, Server发送 Watcher
    - 客户端主动获取
2. 全局配置信息
    - 数据量小
    - 动态变化
    - 集群同步
3. 总结为
    - 配置存储
    - 配置获取
    - 配置变更

### 负载均衡
1. 对资源进行分配负载, 达到优化资源使用, 最大化吞吐率, 最小化响应时间, 避免过载
2. DDNS
    - TODO

### 命名服务
1. 命名服务的资源定位 仅仅 是一个资源标识
    - 一个全局唯一的名字, 类似于数据库的自增id
2. 分布式全局唯一ID的分配机制
    - UUID
        - 太长
        - 含义不明
3. 顺序节点
    - 并且在 api返回值中可以获取这个节点的完整名字
    - 每一个数据节点都可以维护一份子节点的顺序序列
    
### 分布式协调通知
1. Watcher + 异步通知 + 临时节点
    - Mysql_Replicator
        - TODO
2. 通用的分布式系统机器间的通讯方式
    - 心跳检测
        - 在某个节点下创建临时节点
        - 其他机器获取这个节点下的临时节点列表
    - 工作进度汇报
        - 还是 临时临时节点
        - 功能
            - 检测 任务机器是否存活
            - 实时获取任务的执行进度
    - 系统调度
        - 修改节点数据
        - 变更通知所有订阅的客户端

### 集群管理
1. 集群监控: 运行状态的收集
2. 集群控制: 操作与控制
3. 方案
    - 基于 Agent 的分布式管理体系, 在每一个台机器上部署 Agent
        - 大规模升级困难
            - 需要修改大量的 Agent
        - 统一的 Agent 无法满足多样的需求
            - 无法深入应用内部, 与业务耦合紧密的需求无法适应
        - 编程语言的多样性
    - Zookeeper的集群机器存活性监控系统
        - Watcher
        - 临时节点
4. 分布式日志收集系统
    - 需要解决的问题
        - 变化的日志源机器
        - 变化的收集器机器
        - 主要就是: **如何快速 合理 动态 的为每一个收集器分配对应的日志源机器**
    - 收集器机器注册
        - 创建节点 /logs/collector/
        - 机器启动创建节点 /logs/collector/[HostName]
    - 任务分发
        - 日志源机器进行分组
        - 分组后的机器列表分别写到 收集器机器创建的子节点
        - 每个收集器可以获取 需要收集的 日志源机器列表
    - 状态汇报
        - 收集器需要在自己的节点下创建状态子节点
            - /logs/collector/host1/status
            - 类似一种 心跳检测机制
    - 动态分配
        - 收集器机器宕机或者扩容, 需要对收集任务进行动态分配
        - 关注 /logs/collector/ 下的子节点变更
        - 分配方式
            - 全局动态分配
                - 重新分组
            - 局部动态分配
                - 小范围内进行任务的动态分配
                - 收集器汇报日志收集状态, 负载(不止 load, 而是当前收集器任务执行的综合评估)
    - **注意事项**
        - 节点类型
            - 因为在 日志收集场景中, 收集器节点会存放分配给自己的日志源机器列表, 
            - **需要会话失效后, 日志源机器列表不能丢失**
            - 持久节点 + status子节点
        - 日志系统节点监控
            - 收集器正常工作的情况下, status 子节点变化非常频繁, 
            - 放弃监听设置, 采用主动轮询收集节点的策略更好一些
                - 大部分的状态通知是无用的
                - 数量太多
    - 在线云主机管理
        - TODO

### Master 选举
1. 集群的所有机器向数据库中插入一条相同主键Id 的记录
    - 数据库自动做 主键检测
    - **其他机器无法得知 Master 是否存活**
2. **Zookeeper的强一致性: 保证客户端无法重复创建一个已经存在的数据节点**
    - 多个客户端**同时请求创建同一个 临时节点**
    - 创建成功的成为 Master
    - 其他客户端对 Master 创建 Watcher
    - Master 挂掉之后, 其他客户端获取通知, 竞争 Master
    
### 分布式锁
1. 排他锁
    - 独占锁, 写锁
    - 定义锁
        - /exclusive_lock/lock
    - 获取锁
        - 临时节点 --> create("/exclusive_lock/lock")
        - 创建成功表示 该客户端获取锁
        - 其他客户端在 /exclusive_lock 节点注册 子节点变更的 Watcher
    - 释放锁
        - 客户端机器宕机, 临时节点被删除
        - 客户端自己删除
        - 其他客户端获取通知, 发起分布式锁获取
2. 共享锁
    - 读锁
    - create("/share_lock/[HostName]-请求类型-编号")
    - 获取锁
        - 创建临时节点 /share_lock 节点下
    - 判断读写顺序
        - create, 获取所有子节点, 注册 子节点变更 Watcher
        - 确定自己的节点序号在所有子节点的顺序
        - 读请求
            - 没有比自己小的子节点 或者 比自己小的子节点没有读请求 --> 获取锁
        - 写请求
            - 如果自己不是最小的子节点, 锁获取失败, 等待
        - 接到 Watcher, 继续注册 子节点变更 Watcher
    - 释放锁
        - 同上
3. 羊群效应
    - 在整个分布式锁的竞争中, 大量的 Watcher 通知 以及 子节点列表获取的操作
    - 如果 同一时间有多个节点对应的客户端完成事务或者事务中断导致节点消失, Zookeeper 就会
        在短时间 向其余客户端发送大量的通知事件 --> 羊群效应(惊群效应)
4. 优化
    - create("/share_lock/[HostName]-请求类型-编号")
    - **getClildren(), 不注册任何 Watcher**
    - 如果无法获取共享锁, 就调用 exist 来对比 比自己小的那个节点注册 Watcher, 
        - 读
            - 向比自己序号小的最后一个**写**请求节点注册 Watcher
        - 写
            - **向比自己序号小的最后一个节点注册 Watcher**
    - 等待 Watcher
4. 优化的目的
    - **尽量缩小锁的范围**
       